{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BD_CW_Data_preprocessing_and_ML_training_v16_Questions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mithunkumarsr/LearnMLWithMithun/blob/master/BD_CW_Data_preprocessing_and_ML_training_v16_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNBni1yTPbzS"
      },
      "source": [
        "# Big Data Coursework - Resit\n",
        "\n",
        "## Data Processing and Machine Learning in the Cloud \n",
        "\n",
        "This is the **INM432 Big Data resit coursework 2021**. \n",
        "This coursework contains extended elements of **theory** and as well as **practical components** . \n",
        "\n",
        "## Code and Report\n",
        "\n",
        "Your tasks are porting, parallelization, extension, evaluation, and theoretical reflection. \n",
        "The **coding tasks** are to be completed in (a copy of) **this notebook**. \n",
        "Write your code in the **indicated cells** and **include** the **output** in the submitted notebook.  \n",
        "Make sure that your **code contains comments** on its **stucture** and explanations of its **purpose**. \n",
        "\n",
        "Provide also a **report** with the  **textual answers** as indicated **in a separate PDF document**.  \n",
        "Include **screenshots** from the Google Cloud web interface (don't use the SCREENSHOT function that Google provides, but just take a picture of the graphs you see for the VMs) and result tables, as well as written text about the analysis. \n",
        "\n",
        "\n",
        "\n",
        "## Submission\n",
        "\n",
        "Download and submit **the notebook** as an **.ipynb** file and also as a **shareable link** to your notebook (**don’t change the online version after submission**). \n",
        "\n",
        "Further, provide a **PDF report document** with answers containing text, plots, screenshots and tables. **State the number of words** in the document at the end. The report should **not have more than 2000 words**.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNWl-w_Lr-e7"
      },
      "source": [
        "## Introduction and Description\n",
        "\n",
        "This coursework focuses on parallelisation and scalability in the cloud with Spark and TesorFlow/Keras. \n",
        "We start with code based on **lessons 3 and 4** of the [*Fast and Lean Data Science*](https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/fast-and-lean-data-science) course by Martin Gorner. \n",
        "What we will do here is **parallelise** the **pre-processing** and **machine learning** in the cloud and we will perform **evaluation** and **analysis** on the cloud performance, as well as **theoretical discussion**. \n",
        "\n",
        "This coursework contains **4 sections**. Section 0 is only about settings and preparations for cloud computing and has no tasks for you (but do read the code and comments). \n",
        "Section 1 contains tasks about data pre-processing. \n",
        "Sections 2 is about machine learning in the cloud. Section 3 contains the theoretical task for discussion. \n",
        "\n",
        "### Section 0\n",
        "\n",
        "This section just contains some necessary code for setting up the environment.\n",
        "\n",
        "### Section 1 \n",
        "Section 1 is about reading image files into a dataset and creating TF Record files. \n",
        "We will start with an application of some pre-processing steps to a public “Flowers” dataset (3600 images, 5 classes). \n",
        "This is not a vast dataset, but it keeps the tasks more manageable and you can scale up later. \n",
        "\n",
        "In the **Getting Started** we will work through the code from *Fast and Lean Data Science*. There no task for you here, but you should work through this code before you do the rest.\n",
        "\n",
        "In **Task 1** we are going to **measure the speed of reading data** in the cloud and we will **parallelise this in Spark**, using Google Cloud (GC) Dataproc in **Task 2**. \n",
        "For large amounts of training data, the task of creating the TF Record files benefits from parallelisation, which we will also do in Spark in **Task 3**. \n",
        "\n",
        "### Section 2\n",
        "\n",
        "In Section 2, we will **use  pre-processed data** (the public version of the one prepared in Section 1) to train a **deep neural network** in **Tensorflow 2/Keras**. \n",
        "We will use transfer learning to take advantage of existing trained models and reduce the training effort. \n",
        "\n",
        "We will use the GC **AI-Platform** (formerly Cloud ML) in **Task 4**. \n",
        "In contrast to the GC Compute Engine, we can use  machines with GPUs in AI-Platform with free credits. \n",
        "\n",
        "### Section 3\n",
        "\n",
        "This section is about the theoretical discussion, based on to two papers, in **Task 5**. The answers should be given in the PDF report. \n",
        "\n",
        "### General points\n",
        "\n",
        "For **all coding tasks**, take the **time of the operations** and for the cloud operations, get performance **information from the web interfaces** for your reporting and analysis. \n",
        "\n",
        "The **tasks** are **mostly independent**. The later tasks can mostly be addressed without needing the solution to the earlier ones.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89B27-TGiDNB"
      },
      "source": [
        "# Section 0: Set-up\n",
        "\n",
        "You need to run the **imports and authentication every time you work with this notebook**. \n",
        "Use the **local Spark** installation **as needed**. \n",
        "Read through this once and fill in the project ID the first time, then you can just run straight throught them - except for the two authentication cells - at the beginning of each session.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-_ASxZPjqjM"
      },
      "source": [
        "### Imports\n",
        "\n",
        "We import some **packages that will be needed throughout**. \n",
        "For the **code that runs in the cloud**, we will need **separate import sections** that will need to be partly different from the one below. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6rgexPXmY37"
      },
      "source": [
        "import os, sys, math\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.stats\n",
        "import time\n",
        "import datetime\n",
        "import string\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSDlLsAZh_se"
      },
      "source": [
        "### Cloud and Drive authentication\n",
        "\n",
        "This is for **authenticating with with GCS Google Drive**, so that we can create and use our own buckets and access Dataproc and AI-Platform. \n",
        "\n",
        "This section **starts with the two interactive authentications**.\n",
        "\n",
        "First, we mount Google Drive for persistent local storage and create a directory `DB-CW` that you can use for this work. \n",
        "Then we'll set up the cloud environment, including a storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HueTPFr-ZH-P"
      },
      "source": [
        "print('Mounting google drive...')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive\"\n",
        "!mkdir BD-CW\n",
        "%cd \"/content/drive/MyDrive/BD-CW\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2zNNa2xxBcg"
      },
      "source": [
        "Next, we authenticate with the cloud system to enable access to DataProc and AI-Platform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyt_ZnPIRe8K"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5azF8xo3FNC"
      },
      "source": [
        "It is useful to **create a new Google Cloud project** for this coursework. You can do this on the [GC Console page](https://console.cloud.google.com) by clicking on the entry at the top, right of the *Google Cloud Platform* and choosing *New Project*. **Copy** the **generated project ID** to the next cell. Also **enable billing** and the **Compute, Storage and Dataproc** APIs like we did during the labs.\n",
        "\n",
        "We also specify the **default project and region**. The REGION should be `us-central1` as that seems to be the only one that reliably works with the free credit. \n",
        "This way we don't have to specify this information every time we access the cloud.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2GtQXvF2qlS"
      },
      "source": [
        "PROJECT = 'big-data-21-cw' ### Append -xxxx, where xxxx is your City login ###\n",
        "### it seems that the project name here has the be all lower case.\n",
        "!gcloud config set project $PROJECT\n",
        "REGION = 'us-central1'\n",
        "!gcloud config set compute/region $REGION\n",
        "!gcloud config set dataproc/region $REGION\n",
        "\n",
        "!gcloud config list # show some information"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAl4A9PEhPnR"
      },
      "source": [
        "With the cell below, we **create a storage bucket** that we will use later for **global storage**. \n",
        "If the bucket exists you will see a \"ServiceException: 409 ...\", which does not cause any problems. \n",
        "**You must create your own bucket to have write access.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7zaq0gThQRR"
      },
      "source": [
        "BUCKET = 'gs://{}-storage'.format(PROJECT)\n",
        "!gsutil mb $BUCKET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JrBHayWyuS-"
      },
      "source": [
        "The cell below just **defines some routines for displaying images** that will be **used later**. You can see the code by double-clicking, but you don't need to study this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPkvHdAYNt9J"
      },
      "source": [
        "#@title \"Display utility functions [RUN THIS TO ACTIVATE]\" { display-mode: \"form\" }\n",
        "def display_9_images_from_dataset(dataset):\n",
        "  plt.figure(figsize=(13,13))\n",
        "  subplot=331\n",
        "  for i, (image, label) in enumerate(dataset):\n",
        "    plt.subplot(subplot)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image.numpy().astype(np.uint8))   \n",
        "    plt.title(str(label.numpy()), fontsize=16)\n",
        "    # plt.title(label.numpy().decode(), fontsize=16)\n",
        "    subplot += 1\n",
        "    if i==8:\n",
        "      break\n",
        "  plt.tight_layout()\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "\n",
        "def display_training_curves(training, validation, title, subplot):\n",
        "  if subplot%10==1: # set up the subplots on the first call\n",
        "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
        "    plt.tight_layout()\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.set_facecolor('#F8F8F8')\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['train', 'valid.'])\n",
        "\n",
        "def dataset_to_numpy_util(dataset, N):\n",
        "    dataset = dataset.batch(N)\n",
        "    for images, labels in dataset:\n",
        "        numpy_images = images.numpy()\n",
        "        numpy_labels = labels.numpy()\n",
        "        break;\n",
        "    return numpy_images, numpy_labels\n",
        "\n",
        "def title_from_label_and_target(label, correct_label):\n",
        "  correct = (label == correct_label)\n",
        "  return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n",
        "                              CLASSES[correct_label] if not correct else ''), correct\n",
        "\n",
        "def display_one_flower(image, title, subplot, red=False):\n",
        "    plt.subplot(subplot)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
        "    return subplot+1\n",
        "\n",
        "def display_9_images_with_predictions(images, predictions, labels):\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  classes = np.argmax(predictions, axis=-1)\n",
        "  for i, image in enumerate(images):\n",
        "    title, correct = title_from_label_and_target(classes[i], labels[i])\n",
        "    subplot = display_one_flower(image, title, subplot, not correct)\n",
        "    if i >= 8:\n",
        "      break;\n",
        "              \n",
        "  plt.tight_layout()\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9gux-oKnH61"
      },
      "source": [
        "### [optional] Install Spark locally for quick testing \n",
        "\n",
        "You can use the cell below to **install Spark locally on this Colab VM** as in the labs, to do quicker small-scale interactive testing. Using the cloud is still required for the final version. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWbRwh8hnG5h"
      },
      "source": [
        "%cd\n",
        "!apt-get update -qq\n",
        "!apt-get install openjdk-8-jdk-headless -qq\n",
        "#!wget -nv https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar -xzf /content/drive/MyDrive/Big_Data/data/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/root/spark-3.0.1-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "print(pyspark.__version__)\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "print(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvPXiovhi3ZZ"
      },
      "source": [
        "# Section 1: Data pre-processing\n",
        "\n",
        "This section is about the **pre-processing of a dataset** for deep learning with Keras/Tensorflow. \n",
        "The tasks are about **parallelisation** and **analysis** the performance of the cloud services. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EtOSDFbkjgB"
      },
      "source": [
        "## Getting started \n",
        "\n",
        "In this section, we get started based on the code from lecture 3 of the 'Fast and Lean Data Science' course to establish the task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CedEByREXifP"
      },
      "source": [
        "We start by **setting some variables for the *Flowers* dataset**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8K6hL_kiWve"
      },
      "source": [
        "GCS_PATTERN = 'gs://flowers-public/*/*.jpg'\n",
        "SHARDS = 16 # shards are partitiones into shards\n",
        "TARGET_SIZE = [192, 192] # the resolution for the images\n",
        "CLASSES = [b'daisy', b'dandelion', b'roses', b'sunflowers', b'tulips']\n",
        "    # labels for the data (folder names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXD5l2WhktGk"
      },
      "source": [
        "###Read images and labels\n",
        "\n",
        "We **read the image files** from a public GCS bucket that contains the *Flowers* dataset. \n",
        "**TensorFlow**  has **functions** to process standard **image files**. The `decode_jpeg_and_label` function also extracts the label name from the path. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwsZ8X59mu24"
      },
      "source": [
        "nb_images = len(tf.io.gfile.glob(GCS_PATTERN)) \n",
        "shard_size = math.ceil(1.0 * nb_images / SHARDS)\n",
        "print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))\n",
        "\n",
        "def decode_jpeg_and_label(filepath):\n",
        "    bits = tf.io.read_file(filepath)\n",
        "    image = tf.image.decode_jpeg(bits)\n",
        "    # parse flower name from containing directory\n",
        "    label = tf.strings.split(tf.expand_dims(filepath, axis=-1), sep='/')\n",
        "    label2 = label.values[-2]\n",
        "    return image, label2\n",
        "\n",
        "filepathDS = tf.data.Dataset.list_files(GCS_PATTERN) # This also shuffles the images\n",
        "dataset1 = filepathDS.map(decode_jpeg_and_label)\n",
        "for x in dataset1.take(1):\n",
        "    print(x) # show what's in a data item"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgHTJtywiyAB"
      },
      "source": [
        "Now we can **have a look** at the images with one of the display functions. Note the **different aspect ratios**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrRTvVzEOfMl"
      },
      "source": [
        "display_9_images_from_dataset(dataset1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEq3jYKIjGcq"
      },
      "source": [
        "### Resize and crop images to common size\n",
        "\n",
        "Standard Neural Networks, like most machine learning algorithms, need **fixed size input items**. \n",
        "We achieve this here by resizing and cropping the images to a common target resolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6SfG6W-O3Ua"
      },
      "source": [
        "def resize_and_crop_image(image, label):\n",
        "  # Resize and crop using \"fill\" algorithm:\n",
        "  # always make sure the resulting image\n",
        "  # is cut out from the source image so that\n",
        "  # it fills the TARGET_SIZE entirely with no\n",
        "  # black bars and a preserved aspect ratio.\n",
        "  w = tf.shape(image)[0]\n",
        "  h = tf.shape(image)[1]\n",
        "  tw = TARGET_SIZE[1]\n",
        "  th = TARGET_SIZE[0]\n",
        "  resize_crit = (w * th) / (h * tw)\n",
        "  image = tf.cond(resize_crit < 1,\n",
        "                  lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true\n",
        "                  lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false\n",
        "                 )\n",
        "  nw = tf.shape(image)[0]\n",
        "  nh = tf.shape(image)[1]\n",
        "  image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)\n",
        "  return image, label\n",
        "  \n",
        "dataset2 = dataset1.map(resize_and_crop_image)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpwWjVN9PuCt"
      },
      "source": [
        "display_9_images_from_dataset(dataset2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaRNGdocmWHV"
      },
      "source": [
        "Now test continuous reading from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h5oYUC4zrf5"
      },
      "source": [
        "batched_dataset = dataset2.batch(8)\n",
        "sample_set = batched_dataset.take(20)\n",
        "for image, label in sample_set: \n",
        "    print(\"Image batch shape {}, {})\".format(image.numpy().shape,\n",
        "        [lbl.decode('utf8') for lbl in label.numpy()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoGFH0oz5Z6y"
      },
      "source": [
        "## Improving Speed \n",
        "\n",
        "Using individual image files didn't look very fast. The 'Lean and Fast Data Science' course introduced **two techniques to improve the speed**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxOPCfI5dz3"
      },
      "source": [
        "### Recompress the images\n",
        "\n",
        "By **compressing** the images in the **reduced resolution** we save on the size. \n",
        "This **costs some CPU time**, but **saves network and disk bandwith**, especially when the data are **read multiple times**. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duZG0_R65jJe"
      },
      "source": [
        "def recompress_image(image, label):\n",
        "  height = tf.shape(image)[0]\n",
        "  width = tf.shape(image)[1]\n",
        "  image = tf.cast(image, tf.uint8)\n",
        "  image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)\n",
        "  return image, label, height, width\n",
        "\n",
        "dataset3 = dataset2.map(recompress_image)\n",
        "dataset4 = dataset3.batch(shard_size) # sharding: there will be one \"batch\" of images per file "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxSnCKFX6Fv2"
      },
      "source": [
        "# This is a quick test to get an idea how long recompressions takes.  \n",
        "BATCH_SIZE = 8\n",
        "dataset4 = dataset3.batch(BATCH_SIZE)\n",
        "BATCH_NUMBER = 3 # number of batches\n",
        "test_set = dataset4.take(BATCH_NUMBER)\n",
        "for image, label, height, width in test_set:\n",
        "    print(\"Image batch shape {}, {})\".format(image.numpy().shape, [lbl.decode('utf8') for lbl in label.numpy()])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT6qI7FZ6KJW"
      },
      "source": [
        "### Write the dataset to TFRecord files\n",
        "\n",
        "By writing **multiple samples into a single file**, we can make further speed gains.\n",
        "\n",
        "There are **ready-made pre-processed data** versions available, e.g. here: \n",
        "`gs://flowers-public/tfrecords-jpeg-192x192-2/`, that we can use for comparison and later use.\n",
        "\n",
        "First we need to **define a location** where we want to put the file. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be9JCyLk6hyz"
      },
      "source": [
        "GCS_OUTPUT = BUCKET + '/tfrecords-jpeg-192x192-2/flowers'  # prefix for output file names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsaOcjdM6rJc"
      },
      "source": [
        "**[Run only once]** Now we can **write the TFRecord files** to the bucket. \n",
        "\n",
        "Running the cell takes some time and **only needs to be done once** or not at all, as you can use the publicly available data for the next few cells. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYzgji7agVg4",
        "cellView": "both"
      },
      "source": [
        "# functions for writing TFRecord entries\n",
        "# Feature values are always stored as lists, a single data element will be a list of size 1\n",
        "def _bytestring_feature(list_of_bytestrings):\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
        "\n",
        "def _int_feature(list_of_ints): # int64\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))  \n",
        "\n",
        "def to_tfrecord(tfrec_filewriter, img_bytes, label): #, height, width):  \n",
        "  class_num = np.argmax(np.array(CLASSES)==label) # 'roses' => 2 (order defined in CLASSES)\n",
        "  one_hot_class = np.eye(len(CLASSES))[class_num]     # [0, 0, 1, 0, 0] for class #2, roses\n",
        "  feature = {\n",
        "      \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
        "      \"class\": _int_feature([class_num]) #,        # one class in the list\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "  \n",
        "print(\"Writing TFRecords\")\n",
        "tt0 = time.time()\n",
        "filenames = tf.data.Dataset.list_files(GCS_PATTERN) \n",
        "dataset1 = filenames.map(decode_jpeg_and_label)\n",
        "dataset2 = dataset1.map(resize_and_crop_image)  \n",
        "dataset3 = dataset2.map(recompress_image)\n",
        "dataset3 = dataset3.batch(shard_size) # sharding: there will be one \"batch\" of images per file \n",
        "for shard, (image, label, height, width) in enumerate(dataset3):\n",
        "  # batch size used as shard size here\n",
        "  shard_size = image.numpy().shape[0]\n",
        "  # good practice to have the number of records in the filename\n",
        "  filename = GCS_OUTPUT + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n",
        "  # You need to change GCS_OUTPUT to your own bucket to actually create new files  \n",
        "  with tf.io.TFRecordWriter(filename) as out_file:\n",
        "    for i in range(shard_size):\n",
        "      example = to_tfrecord(out_file,\n",
        "                            image.numpy()[i], # re-compressed image: already a byte string\n",
        "                            label.numpy()[i] #, height.numpy()[i], width.numpy()[i]\n",
        "                            )\n",
        "      out_file.write(example.SerializeToString())\n",
        "    print(\"Wrote file {} containing {} records\".format(filename, shard_size))\n",
        "print(\"Total time: \"+str(time.time()-tt0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1zyN9txTUuX"
      },
      "source": [
        "### Test the TFRecord files\n",
        "\n",
        "We can now **read from the TFRecord files**. By default, we use the files in the public bucket. Comment out the 1st line of the cell below to use the files written in the cell above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xp7Z3DlTSBe"
      },
      "source": [
        "GCS_OUTPUT = 'gs://flowers-public/tfrecords-jpeg-192x192-2/' # remove/change to use your own files\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    features = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string = bytestring (not text string)\n",
        "        \"class\": tf.io.FixedLenFeature([], tf.int64) #,   # shape [] means scalar\n",
        "    }\n",
        "    # decode the TFRecord\n",
        "    example = tf.io.parse_single_example(example, features)\n",
        "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
        "    image = tf.reshape(image, [*TARGET_SIZE, 3])    \n",
        "    class_num = example['class']\n",
        "    return image, class_num \n",
        "\n",
        "def load_dataset(filenames):\n",
        "  # read from TFRecords. For optimal performance, read from multiple    \n",
        "  # TFRecord files at once and set the option experimental_deterministic = False\n",
        "  # to allow order-altering optimizations.\n",
        "  option_no_order = tf.data.Options()\n",
        "  option_no_order.experimental_deterministic = False\n",
        "\n",
        "  dataset = tf.data.TFRecordDataset(filenames)\n",
        "  dataset = dataset.with_options(option_no_order)\n",
        "  dataset = dataset.map(read_tfrecord)\n",
        "  return dataset\n",
        "\n",
        "    \n",
        "filenames = tf.io.gfile.glob(GCS_OUTPUT + \"*.tfrec\")\n",
        "datasetDecoded = load_dataset(filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vbh3PndkSK4"
      },
      "source": [
        "display_9_images_from_dataset(datasetDecoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC98hPqGpugm"
      },
      "source": [
        "Let's have a look **if reading from the TFRecord** files is **quicker**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsn5vNrdpM4B"
      },
      "source": [
        "batched_dataset = datasetDecoded.batch(8)\n",
        "sample_set = batched_dataset.take(20)\n",
        "\n",
        "for image, label in sample_set: \n",
        "    print(\"Image batch shape {}, {})\".format(image.numpy().shape, \\\n",
        "                        [str(lbl) for lbl in label.numpy()]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO9XTV0jLg0a"
      },
      "source": [
        "Wow, we have a **massive speed-up**! The repackageing is worthwhile :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx00RcJNkS-f"
      },
      "source": [
        "## Task1: Cloud Speed tests (8%)\n",
        "\n",
        "We have just seen that **reading from the pre-processed TFRecord files** is **faster** than reading individual image files and decoding on the fly. \n",
        "This task is about **understanding and quantifying this effect** better. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw0Bi9Di4c71"
      },
      "source": [
        "### 1a): Speed test implementation (5%)\n",
        "\n",
        "Start by implementing **code for time measurement** to determine the **throughput in images per second**. \n",
        "Don't render the images, just print some basic information in order to **avoid measuring image rendering time**. \n",
        "It's useful to write the information to the null device for longer measurements (e.g. \n",
        "`print('string', file=open(\"/dev/null\", mode='w')`). \n",
        "That way it will not clutter your cell output.\n",
        "\n",
        "Use batches ( `dataset.batch(batch_size)` ) and select a sample with (`dataset.take(batch_number)`). \n",
        "Good starting values are 8 and 10, i.e. we take 10 batches of 8 samples each. \n",
        "\n",
        "Use the `time.time()` to take the **time measurement** and take it multiple times, reading from the same dataset. \n",
        "\n",
        "**Vary** the size of the batch (`batch_size`) and the number of batches (`batch_number`) and **store the results for different values**. \n",
        "Store also the **results for each repetition** over the same dataset (repeat 2 or 3 times).\n",
        "\n",
        "The speed test should be packaged into a **function** `time_configs()` that takes a configuration, i.e. dataset and the batch_sizes, batch_numbers, and number of repetitions, as **arguments** and runs the repeated time measurements. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZlwZop5tsqs"
      },
      "source": [
        "# Here are some useful values for testing\n",
        "#batch_sizes = [2,4] \n",
        "#batch_number = [3,6] \n",
        "#repetitions = [1,2] \n",
        "\n",
        "### CODING TASK ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IICIJYaiVsLz"
      },
      "source": [
        "**Try your function** with a **small number** of configurations of batch_sizes batch_numbers and repetions, so that we get a set of parameter combinations and corresponding reading speeds.\n",
        "Try reading from the image files (dataset2) and the TFRecord files (datasetDecoded).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SwjzQtfj-PE"
      },
      "source": [
        "### CODING TASK ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gP-o62HkBVL"
      },
      "source": [
        "### 1b) Speed test analysis (3%)\n",
        "\n",
        "Do a **linear regression** over pre-processing and reading **speed** as a **function of parameter values**. Look at the parameters `batch_size`, `batch_number`, `repetition` and the dataset size (`batch_size * batch_number`). \n",
        "\n",
        "This needs to be done **separately for the two datasets**. \n",
        "\n",
        "**Print** the regression results (slope, intercept, pvalue etc) and **plot** the measurements and the regression line for each of the 4 parameters. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF1WA2Df6_28"
      },
      "source": [
        "### CODING TASK ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQEjuE-bD5W9"
      },
      "source": [
        "## Task 2: Parallelising the speed test with Spark in the cloud. (32%)\n",
        "\n",
        "As an exercise in **Spark programming and optimisation** as well as **performance analysis**, we will now implement the **speed test** with multiple parameters in parallel with Spark. \n",
        "Runing mutliple tests in parallel would **not be a useful approach on a single machine, but it can be in the cloud** (you will be asked to reason about this later). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luldPUhppn2c"
      },
      "source": [
        "### Preparation: Running Spark in the cloud \n",
        "\n",
        "In order to test multiple configurations, especially with reasonably large datasets, we will use **Spark** to run **multiple tests in parallel**. \n",
        "We will start by looking at **how to use Spark on GCS Dataproc**. \n",
        "\n",
        "This section shows you in detail **how to run Python code in Dataproc**. \n",
        "You may need to **enable the Dataproc API** on the [console Dataproc page](https://console.cloud.google.com/dataproc/clusters/), if you have not done so, yet. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELL92lwEs7ju"
      },
      "source": [
        "First we need to **create a cluster**. We start with a single machine, just to try it out. \n",
        "\n",
        "We are using the `gcloud dataproc clusters` command. [Click here for documentation](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters). \n",
        "The **parameter** `--image-version 1.4-ubuntu18` makes sure we get **intended software**. \n",
        "\n",
        "Starting a cluster **can take a few minutes**. You can wait for the cell to finish processing or interrupt its execution and check on the [console Dataproc page](https://console.cloud.google.com/dataproc/clusters/) if the cluster is ready.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt9XM8yYtRjJ"
      },
      "source": [
        "CLUSTER = '{}-cluster'.format(PROJECT)\n",
        "!gcloud dataproc clusters create $CLUSTER \\\n",
        "    --image-version 1.4-ubuntu18 --single-node \\\n",
        "    --master-machine-type n1-standard-2 \\\n",
        "    --master-boot-disk-type pd-ssd --master-boot-disk-size 100 \\\n",
        "    --max-idle 3600s "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjt47GKUWFHQ"
      },
      "source": [
        "The `--max-idle 3600s` flag means that the cluster will be **deleted automatically** once it has been **idle for 1 hour**. This helps minimising costs for a cluster left running by accident. \n",
        "\n",
        "This is a single-node cluster. This is created a bit more quickly than a multi-node cluster, but set-up is still quite slow (several minutes) because of the **restrictions on the free tier**. \n",
        "You **can switch to paid tier** and still use your free credit, but then you **may use up your free credit more quickly** and have to **pay for usage**.\n",
        "If you stay in the free tier Google promises not to charge you. The free tier resources are sufficient for this coursework, if you use the local Spark installation for testing and set parameters with care. All the development of this coursework was done with a free tier account. \n",
        "\n",
        "We have not specified the region (we could have used `--region $REGION`) as we set already the default for Dataproc in the beginning. \n",
        "\n",
        "You can run the **command below to get extensive information** about your running cluster. \n",
        "However, it is usually **more practical** to look at the [console Dataproc page](https://console.cloud.google.com/dataproc/clusters/). \n",
        "You can check the details and current state of your cluster by clicking on its name. \n",
        "Double-check there at the end of your working session to make sure that no clusters are left running, especially when you are not in free mode any more. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnymMq3MWnQX"
      },
      "source": [
        "!gcloud dataproc clusters describe $CLUSTER "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-A4ZVjC2drX"
      },
      "source": [
        "Now that our cluster is running, we can submit a Spark job. A minimal Spark job is just a Python script. A simple \"Hello World\" Spark script is provided in a public cloud bucket. Let's have a look at it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITTVHjWWXQsL"
      },
      "source": [
        "!gsutil cat gs://dataproc-examples/pyspark/hello-world/hello-world.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2Yz0RxVXSzD"
      },
      "source": [
        "... and run it on the cluster. We submit the job with the `gcloud dataproc jobs` command ([click here for the documentation](https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs)) with the cluster name.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H9IsWYMVlDq"
      },
      "source": [
        "!gcloud dataproc jobs submit pyspark --cluster $CLUSTER \\\n",
        "    gs://dataproc-examples/pyspark/hello-world/hello-world.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiNBAoQiXDe0"
      },
      "source": [
        "The `trackingUrl` shown above will only work as long as the job is running. On to the [dataproc page](https://console.cloud.google.com/dataproc/clusters/), you can click through to your cluster page and from there to your job page to see details on past jobs.\n",
        "\n",
        "You may get some warnings like this: \n",
        "`WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair' ...\n",
        "or this one:\n",
        "`WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception`. There was not enough time to research these fully but they don't affect what we do here and can be ignored. \n",
        "\n",
        "One issue is that we need to **get output data from the cluster back to the notebook**. We can output text through printing into the output stream, but that does not work well for scaling up, for automation or for binary data. \n",
        "\n",
        "A better solution is to **pass an argument** to the job, to tell the job on the cluster **where to write the output**. \n",
        "This requires a bit of extra code as shown below using the argparse package.\n",
        "The example below the checks if the script runs in Colab and if that is the case, it does not execute the run function. \n",
        "This is useful for quick testing with a local spark instance.\n",
        "\n",
        "For running the file in DataProc, write to a local file `hello-world.py`, uncomment the  the `%%writefile` magic as in the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvcr5pQ8WCeJ"
      },
      "source": [
        "#%%writefile hello-world.py\n",
        "\n",
        "import sys\n",
        "import pyspark\n",
        "import argparse\n",
        "import pickle \n",
        "\n",
        "def save(object,bucket,filename):\n",
        "    with open(filename,mode='wb') as f:\n",
        "        pickle.dump(object,f)\n",
        "    print(\"Saving {} to {}\".format(filename,bucket))\n",
        "    import subprocess\n",
        "    proc = subprocess.run([\"gsutil\",\"cp\", filename, bucket],stderr=subprocess.PIPE)\n",
        "    print(\"gstuil returned: \" + str(proc.returncode))\n",
        "    print(str(proc.stderr))\n",
        "\n",
        "def runWordCount(argv): \n",
        "    # Parse the provided arguments\n",
        "    print(argv)\n",
        "    parser = argparse.ArgumentParser() # get a parser object\n",
        "    parser.add_argument('--out_bucket', metavar='out_bucket', required=True,\n",
        "                        help='The bucket URL for the result.') # add a required argument\n",
        "    parser.add_argument('--out_file', metavar='out_file', required=True,\n",
        "                        help='The filename for the result.') # add a required argument\n",
        "    args = parser.parse_args(argv) # read the value\n",
        "    # the value provided with --out_bucket is now in args.out_bucket\n",
        "    sc = pyspark.SparkContext.getOrCreate()\n",
        "    rdd = sc.parallelize(['Hello,', 'world!'])\n",
        "    words = sorted(rdd.collect())\n",
        "    save(words,args.out_bucket,args.out_file)\n",
        "\n",
        "if  'google.colab' not in sys.modules: # Don't use system arguments run in Colab \n",
        "    runWordCount(sys.argv[1:])  \n",
        "elif __name__ == \"__main__\" : # but define them manually\n",
        "    runWordCount([\"--out_bucket\", BUCKET, \"--out_file\", \"words.pkl\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl72JssUGyoi"
      },
      "source": [
        "**Once the code works as intended**, you can write it to the local disk (on your Colab instance). For this, **uncomment the first line with the `%%writefile` magic** and then **re-run the cell**. \n",
        "\n",
        "Then **check** that the file **is in the current directory** and **has the right contents** like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUDTWpVpG3oZ"
      },
      "source": [
        "!pwd\n",
        "!ls -l hello-world.py\n",
        "!cat hello-world.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnS846qFC2B9"
      },
      "source": [
        "We can now submit the job with an extra section for application arguments. It's started by `--`, which indicates that all following arguments are to be sent to our Spark application. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoI9owUfuGGJ"
      },
      "source": [
        "FILENAME = 'words.pkl'\n",
        "!gcloud dataproc jobs submit pyspark --cluster $CLUSTER --region $REGION \\\n",
        "    ./hello-world.py \\\n",
        "    -- --out_bucket $BUCKET --out_file $FILENAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPvy_iDTPZTo"
      },
      "source": [
        "Once the job has finished, we can **use the output** by **copying it from the bucket** and **reading it as a local file**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD0jG0YiBV_l"
      },
      "source": [
        "# Make sure you are writing to the right directory\n",
        "%cd /content/drive/MyDrive/BD-CW\n",
        "!gsutil cp $BUCKET/$FILENAME . \n",
        "!ls -l\n",
        "with open(FILENAME,mode='rb') as f:\n",
        "    words = pickle.load(f) \n",
        "\n",
        "print(\"Content of {} : {}\".format(FILENAME,words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXYVsztX2uyM"
      },
      "source": [
        "At the end of a session we should **delete the cluster**, as it incurs a **cost for the time** it runs.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6LMV7cOuTQK"
      },
      "source": [
        "!gcloud dataproc clusters delete $CLUSTER -q \n",
        "# the -q flag disables the confirmation prompt \n",
        "# , we want to make sure it really gets deleted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fJFroJzVal-"
      },
      "source": [
        "### 2a) Create the script (12%)\n",
        "Your task is now to **port the speed test to Spark** for running it in the cloud in DataProc. \n",
        "**Adapt the preprocessing code** as a Spark programm that performs the same actions as above, but **with Spark RDDs in a distributed way**. \n",
        "The distribution should be such that **each parameter combination (except repetition)** is processed in a separate Spark task.\n",
        "\n",
        "More specifically: \n",
        "*   i) combine the previous cells to have the code to create a dataset and create a list of parameter combinations in an RDD (2%)\n",
        "*   ii) get a spark context and  create the dataset and run timing test for each combination in parallel (2%)\n",
        "*   iii) transform the resulting RDD to the structure \\( parameter_combination, images_per_second \\) and save these values in an array (2%)\n",
        "*   iv) create an RDD with all results for each parameter as (parameter_value,images_per_second) and collect the result for each parameter  (1%)\n",
        "*   v) create an RDD with the average reading speeds for each paramter value and collect the results. Keep associativity in mind when implementing the average. (3%) \n",
        "*   vi) write the results to a pickle file in your bucket (1%)\n",
        "*   vii) Write your code it into a file using the *cell magic* `%%writefile spark_job.py` (1%)\n",
        "\n",
        "\n",
        "**Important:** The task here is not to parallelize the pre-processing, but to run multiple speed tests in parallel using Spark.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP2LEeKMGv4v"
      },
      "source": [
        "### CODING TASK ###\n",
        "## Take the timing for reading with multiple configurations in parallel with Spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ts44bFzGw2a"
      },
      "source": [
        "### 2b) Set up a cluster and run the script. (4%)\n",
        "\n",
        "In the free credit tier on Google Cloud, there are the following **restrictions**:\n",
        "- max 100GB of *SSD persistent disk*\n",
        "- max 2000GB of *standard persisten disk* \n",
        "- max 8 *vCPU*s\n",
        "\n",
        "See [here](https://cloud.google.com/free/docs/gcp-free-tier#free-trial) for details \n",
        "The disks are virtual disks, where the I/O speed is limited in proportion to the size, so that we should allocated them evenly.\n",
        "This has mainly an effect on the time the cluster needs to start, as we are reading the data mainly from the bucket and we are not writing much to disk at all.   \n",
        "\n",
        "The largest possible cluster within these constraints has 1 master and 7 worker nodes. \n",
        "Each of them with 1 (virtual) CPU. The master should get the full *SSD* capacity and the 7 worker nodes should get equal shares of the *standard* disk capacity to maximise throughput. \n",
        "\n",
        "In order to run our code on the cluster, we need to **make sure** that **TensorFlow 2 is installed**. The Colab magic  `%tensorflow_version 2.x` will not work within Spark. \n",
        "Instead, enable package installation by passing a flag `--initialization-actions` with argument `gs://goog-dataproc-initialization-actions-$REGION/python/pip-install.sh` (this is a public script that will read metadata to determine which packages to install). \n",
        "The packages are then specified by providing a `--metadata` flag with the argument `PIP_PACKAGES=tensorflow==2.1.0`. \n",
        "(2%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOiK8N66crOd"
      },
      "source": [
        "### CODING TASK ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuXtPhZi2Ghp"
      },
      "source": [
        "Once the cluster is running, you can run the job. It is useful to create a new filename, so that results don't get overwritten. \n",
        "\n",
        "You can for instance use `str(datetime.datetime.now().strftime(\"%y%m%d-%H%M\")` to get a string with the current date and time and use that in the file name.\n",
        "\n",
        "While you run this job, switch to the DataProc web page and take images of the CPU and Network load over time. \n",
        "Again, don't use the SCREENSHOT function that Google provides, but just take a picture of the graphs you see for the VMs. These will be needed in the next task. \n",
        "(2%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBKW10BsCPAo"
      },
      "source": [
        "### CODING TASK ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFVwxfjQsV_P"
      },
      "source": [
        "### 2c) Improve cluster efficiency (6%)\n",
        "\n",
        "If you implemented a straightfoward version, you will probably have some inefficiencies in your code. \n",
        "There are two main aspects: \n",
        "- Caching \n",
        "- Parallelisation branches\n",
        "\n",
        "i) Because we are reading multiple times from the RDD to read the valudes for the different parameters and their averages, caching existing results is important. Explain where in the process caching can help, and add it to your code, if you haven't yet. Measure the the effect of using caching or not using it.\n",
        "\n",
        "ii) You will probably observe that all the computation is done on only two branches, if you do the straightforward implementation. This can be adressed by using the second parameter in the initial call parallelize. \n",
        "\n",
        "Write make the suitable changes in the code you have written above and mark them up in comments as `### TASK 2Ci ###` and  `### TASK 2cii ###`. \n",
        "\n",
        "Explain in your report what the reasons for these changes are and demonstrate and interpret their effects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_uNa6VeTLSN"
      },
      "source": [
        "### 2d) Retrieve, analyse and discuss the output (10%)\n",
        "\n",
        "\n",
        "Implement a linear regression over each parameter for the two cases (reading from image files/reading TFRecord files. \n",
        "List a table with the output and interpret the results in terms of the effects of overall.  \n",
        "Also, plot the output values, the averages per parameter value and the regression lines for each parameter and for the product of batch_size and batch_number\n",
        "\n",
        "Discuss the implications of this result for large-scale machine learning. Keep in mind that cloud data may be stored in distant physical locations. Use the numbers provided in the PDF latency-numbers provided on Moodle or available [here](https://gist.github.com/hellerbarde/2843375) for your arguments. \n",
        "\n",
        "How is the observed behaviour similar or different from what you’d expect from a single machine? Why would cloud providers tie throughput to capacity of disk resources? \n",
        "\n",
        "By parallelising the speed test we are making assumptions about the limits of the bucket reading speeds. \n",
        "See [here](https://cloud.google.com/storage/docs/request-rate) for more information.\n",
        "Discuss, what we need to consider in running tests in parallel on the cloud, which bottlenecks we might be identifying, and how this relates to your results. \n",
        "\n",
        "Discuss to what extent linear modelling reflects the effects we are observing. Discuss what could expected from a theoretical perspective and what can be useful in practice.\n",
        "  \n",
        "\n",
        "Write your code below and include the output in your submission. Provide the answer text in your report. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np2aVJ28XidM"
      },
      "source": [
        "### CODING TASK ### "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n90Cpq_L-fht"
      },
      "source": [
        "## Task 3: Write TFRecord files to the cloud with Spark (20%)\n",
        "\n",
        "This task of pre-processing images and creating TFRecord files is a more straightforward case of parallelisation. We will again use Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCpEv66mgjfQ"
      },
      "source": [
        "### 3a)\tCreate the script (10%)\n",
        "\n",
        "Re-Implement the code for creating the TFRecord files in Spark, using Spark mechanisms for distributing the workload over the shards. \n",
        "\n",
        "You need to copy over the mapping functions and adapt the resizing and recompression function to Spark (onyl one argument). (2%)\n",
        "Replace the TensorFlow Dataset objects with RDDs, starting with an RDD that contains the list of image filenames. (2%)\n",
        "Then use the mapping functions like with the TensorFlow Dataset object. (2%)\n",
        "\n",
        "The code for writing to the TFRecord files needs to be put into a function, that can be applied to every partition with the ['RDD.mapPartitionsWithIndex'](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapPartitionsWithIndex) function. The return value of that function is not essential. You can return the filename, so that you have a list of the created TFRecord files. (4%) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocxFfbNQ8Fhm"
      },
      "source": [
        "### CODING TASK ###\n",
        "## Write the images to TFRecord files with Spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ggSOXhehVN9"
      },
      "source": [
        "### 3b)\tRun and test (2%)\n",
        "\n",
        "Run the script and test the output. Demonstrate that the files you wrote with your spark job work can be read and used by the model training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs4B5egy-H6B"
      },
      "source": [
        "### CODING TASK ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHG5TzULkJh1"
      },
      "source": [
        "Read from the TFRecord Dataset to test. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crKBTwVXLBWm"
      },
      "source": [
        "### CODING TASK ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mywE0vi1g0O9"
      },
      "source": [
        "### 3c)\tExperiment and discussion (8%)\n",
        "\n",
        "i) Explain the difference between this use of Spark and most standard applications like e.g. in our labs. (2%)\n",
        "\n",
        "ii)\tTest your program with 4 machines with double the resources each (2 vCPUs, memory, disk) and 1 machine with 8-fold resources (8 vCPUs). Discuss the results in terms of disk I/O and network bandwidth allocation in the cloud. (6%)\n",
        "\n",
        "Write your answers in your report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne3F4KQmwZsn"
      },
      "source": [
        "# Section 2: Machine Learning in the Cloud\n",
        "\n",
        "In this section we will use the pre-processed data with the GC AI-Platform for Machine Learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCEIN39fxafa"
      },
      "source": [
        "## Preparation: Machine Learning Setup\n",
        "\n",
        "As in Section 1, In this section, we get started based on  code from the 'Fast and Lean Data Science' course, this time lecture 4, to establish the task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zTUtAhPur3-"
      },
      "source": [
        "### Create Train and Test Sets \n",
        "\n",
        "We will first set up a few variables for the machine learning and then run split the test and training sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8P7UUROKt0e"
      },
      "source": [
        "BATCH_SIZE = 64 # this is a good setting for the Colab GPU (K80)\n",
        "#BATCH_SIZE = 128 # works on newer GPUs with more memory (available on GCS AI-Platform) \n",
        "EPOCHS = 5 # this value is for testing. Increase later \n",
        "GCS_TFR_PATTERN = 'gs://flowers-public/tfrecords-jpeg-192x192-2/*.tfrec' \n",
        "# this is a link to public data, use your own later\n",
        "#GCS_TFR_PATTERN = 'gs://bd-cw-2-bucket/tfrecords-jpeg-192x192-2/flowers*.tfrec' \n",
        "\n",
        "TARGET_SIZE = [192,192]\n",
        "\n",
        "VALIDATION_SPLIT = 0.25\n",
        "\n",
        "CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # maps class numbers to labels strings\n",
        "\n",
        "SAMPLE_NUM=3670 # size of the Flowers dataset, change as appropriate for smaller samples/different datasets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2db_0G3Gjf1"
      },
      "source": [
        "# splitting data files between training and validation\n",
        "filenames = tf.io.gfile.glob(GCS_TFR_PATTERN)\n",
        "print(\"len(filenames): \"+str(len(filenames)))\n",
        "split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = filenames[split:]\n",
        "validation_filenames = filenames[:split]\n",
        "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\n",
        "validation_steps = int(SAMPLE_NUM // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(SAMPLE_NUM // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
        "print(\"With a batch size of {}, there will be {} batches per training epoch and {} batch(es) per validation run.\".format(BATCH_SIZE, steps_per_epoch, validation_steps))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVrWIxZeve9f"
      },
      "source": [
        "def get_batched_dataset(filenames, train=False):\n",
        "  dataset = load_dataset(filenames)\n",
        "  dataset = dataset.cache() # This dataset fits in RAM\n",
        "  if train:\n",
        "    # Best practices for Keras:\n",
        "    # Training dataset: repeat then batch\n",
        "    # Evaluation dataset: do not repeat\n",
        "    dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "  # should shuffle too but this dataset was well shuffled on disk already\n",
        "  return dataset\n",
        "  # source: Dataset performance guide: https://www.tensorflow.org/guide/performance/datasets\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset = get_batched_dataset(training_filenames, train=True)\n",
        "validation_dataset = get_batched_dataset(validation_filenames, train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYERciZ_8g7C"
      },
      "source": [
        "sample_set = training_dataset.take(4)\n",
        "for image, label in sample_set: \n",
        "    print(\"Image batch shape {}, {})\".format(image.numpy().shape, \\\n",
        "                        [str(lbl) for lbl in label.numpy()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHSjcF-8xFsU"
      },
      "source": [
        "### Set up a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVObGR7ows3V"
      },
      "source": [
        "pretrained_model = tf.keras.applications.MobileNetV2(input_shape=[*TARGET_SIZE, 3], include_top=False)\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100,activation='relu'),\n",
        "    tf.keras.layers.Dropout(.5),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or7ddM0XxRsS"
      },
      "source": [
        "### Local Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER5jkb2oxRHT"
      },
      "source": [
        "tt0 = time.time()\n",
        "history = model.fit(training_dataset, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n",
        "                    validation_data=validation_dataset, validation_steps=validation_steps)\n",
        "tt = time.time() - tt0\n",
        "print(\"Wall clock time = {}\".format(tt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLZT8CBxLT0j"
      },
      "source": [
        "Result: Training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDmcODmE_tH7"
      },
      "source": [
        "print(history.history.keys())\n",
        "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
        "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76dqrxNj_7SR"
      },
      "source": [
        "# random input: execute multiple times to change results\n",
        "flowers, labels = dataset_to_numpy_util(load_dataset(validation_filenames).skip(np.random.randint(300)), 9)\n",
        "\n",
        "predictions = model.predict(flowers, steps=1)\n",
        "print(np.array(CLASSES)[np.argmax(predictions, axis=-1)].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KwHUcb3Sppg"
      },
      "source": [
        "display_9_images_with_predictions(flowers, predictions, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FlcfkFEVvbi"
      },
      "source": [
        "## Task 4: Machine learning in the cloud (20%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNHg5kP0V57z"
      },
      "source": [
        "### 4a) Create the package and code for the AI-Platform (7%)\n",
        "\n",
        "The AI Platform needs code in a 'package'. \n",
        "The package can contain complex software systems and extensive  information on the setup. \n",
        "We will keep the package as simple as possible here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xL6elRkh9Dh"
      },
      "source": [
        "i) The minimal 'package' in Python needs a directory with two modules (i.e. Python files). \n",
        "Create a directory 'trainer' and then an empty file `trainer/__init__.py`. \n",
        "This can be done by using the command line tool `touch`. \n",
        "Check that the file exists. (1%) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0OHwcHGV3hc"
      },
      "source": [
        "### CODING TASK ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFLmxBRFY-Up"
      },
      "source": [
        "ii) The other file we need is the modlue with the training code, which we will call `task.py`. \n",
        "\n",
        "We can build this file by combining the last few cells under 'Machine Learning Setup' into one file, almost literally. \n",
        "\n",
        "Then, we need to save the model itself and the training history into a file after training. \n",
        "The `history` object returned by `model.fit` can not be pickled, but it's data in `history.history` can. \n",
        "For saving the model, use `model.save()`.\n",
        "Write the necessary code, like in section 2 when preparing jobs for Spark/DataProc.\n",
        "\n",
        "If you use argparse as in section 2 (recommended), then you need to add an argument `--job-dir` that will be passed through from the AI-Platform to your program.\n",
        "(6%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtdNehRha60o"
      },
      "source": [
        "#%%writefile trainer/task.py\n",
        "### CODING TASK ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DLLJRcl7XF-"
      },
      "source": [
        "### 4b) Run the Training on the AI-Platform and view the Output (3%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS2IRbpacb1P"
      },
      "source": [
        "Now, with all code in place, we can submit the package. AI-Platform is serverless, therefore we don't need to create a cluster ourselves, but we just submit a job and the master and workers will be set up automatically. \n",
        "Find [here](https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training) the information how to submit a training job in GCS AI-Platform.  \n",
        "\n",
        "The job will wait in a queue before it gets executed, this can take several minutes (at least on free credits). \n",
        "It's therefore a good idea to test the script locally before you submit to the cloud. \n",
        "\n",
        "You can, however, get powerful hardware. Up to 30 K80 or even P100 GPUs are available, but not all configurations are possible. \n",
        "Check [here](https://cloud.google.com/ai-platform/training/docs/quotas#gpu-quota) and [here](https://cloud.google.com/ai-platform/training/docs/using-gpus), and test [here on the console](https://console.cloud.google.com/ai-platform/create-job/custom-code?folder=true&organizationId=true) with the Create option whether your configuration works in the Google Cloud.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHc6jn1FHNZQ"
      },
      "source": [
        "# AI Platform parameters\n",
        "TRAINER_PACKAGE_PATH=\"trainer\"\n",
        "MAIN_TRAINER_MODULE=\"trainer.task\"\n",
        "PACKAGE_STAGING_PATH=BUCKET\n",
        "JOB_NAME=\"flowers_training\" # you need a new job name for every run\n",
        "JOB_DIR=BUCKET+'/jobs/'+JOB_NAME\n",
        "\n",
        "### CODING TASK ### "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5UtJUvvHOLi"
      },
      "source": [
        "!gcloud ai-platform jobs stream-logs $JOB_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vNEouyOiIsj"
      },
      "source": [
        "After training, plot the accuracy and loss curves based on the saved history."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Y_xBo_Hqgc"
      },
      "source": [
        "### CODING TASK ### "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XOSExP4hgcW"
      },
      "source": [
        "### 4c) Distributed learning\t(10%)\n",
        "\n",
        "Apply a distributed learning strategy to the code (see https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras?hl=nb#choose_the_right_strategy for an example ).  \n",
        "Add the necessary changes to the code.\n",
        "\n",
        "Experiment with different strategies and batch sizes. Report and interpret your results. \n",
        "For the defining cluster sizes, you can use command line options as described here: https://cloud.google.com/ai-platform/training/docs/machine-types#legacy-machine-types . \n",
        "Check the pricing here ( https://cloud.google.com/ai-platform/training/pricing ) and make sure everything works before you run jobs on the expensive configurations. \n",
        "\n",
        "The machines with P100 and V100 GPUs are quite expensive. \n",
        "For the experiments here, the `standard_gpu` (1xK80) and the `complex_model_l_gpu` (8xK80) are sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlrDmczWmt_J"
      },
      "source": [
        "### CODING TASK ### \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmXhJrpza5U0"
      },
      "source": [
        "After you have run the experiments, copy the results over from the bucket to the local file system, so that you can extract the values for a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG9Nr6s7BzCa"
      },
      "source": [
        "### CODING TASK ### \n",
        "# Read output from jobs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsvglpSzm78C"
      },
      "source": [
        "# Section 3. Theoretical discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYLysXrJm-9M"
      },
      "source": [
        "## Task 5: Discussion in context. (20%)\n",
        "\n",
        "In this task we refer to two ideas,what are introduced in these two papers:\n",
        "-\tAlipourfard, O., Liu, H. H., Chen, J., Venkataraman, S., Yu, M., & Zhang, M. (2017). [Cherrypick: Adaptively unearthing the best cloud configurations for big data analytics.](https://people.irisa.fr/Davide.Frey/wp-content/uploads/2018/02/cherrypick.pdf). In USENIX NSDI  17 (pp. 469-482).\n",
        "-\tSmith, S. L., Kindermans, P. J., Ying, C., & Le, Q. V. (2018). [Don't Decay the Learning Rate, Increase the Batch Size.](https://openreview.net/pdf?id=B1Yy1BxCZ) In ICLR (no pagination).\n",
        "\n",
        "Alipourfard et al (2017) introduce  the prediction an optimal or near-optimal cloud configuration for a given task. \n",
        "Smith et al (2018) introduce the concept of varying the batch size during the training of neural networks. \n",
        "\n",
        "### 5a)\tContextualise\n",
        "\n",
        "Relate the previous tasks and the results to these two concepts. (It is not necessary to work through the details of the papers, focus just on the main ideas). To what extent and under what conditions do the concepts and techniques in the paper apply for this task? (10%)\n",
        "\n",
        "### 5b)\tStrategise\n",
        "\n",
        "Define - as far as possible - concrete strategies for different application scenarios (batch, on-line, stream) discuss general relationship with the two concepts above. (10%)\n",
        "\n",
        "Provide the answers to these questions in your report. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NAZ7LeUBuzB"
      },
      "source": [
        "## Final cleanup\n",
        "\n",
        "Once you have finshed the work, you can delete the buckets, to stop incurring cost that depletes your credit. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARDdlQIPBvTi"
      },
      "source": [
        "!gsutil -m rm -r $BUCKET/* # Empty your bucket \n",
        "!gsutil rb $BUCKET # delete the bucket"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}